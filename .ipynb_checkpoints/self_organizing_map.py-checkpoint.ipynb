{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organizing Map: Unsupervised/Dimensionality Reduction\n",
    "\n",
    "### Custom python implementation of SOM \n",
    "#### Tested with the usual suspect: color map reduced to 2D grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self organizing map is a wonderful way to figure out patterns within data set provided. It creates a network (dimensions given by user, usually is 2D for easier visualization) where similar datasets cluster together.\n",
    "\n",
    "\n",
    "The way SOM works is given below:\n",
    "\n",
    "1. Your input is a dataset of the dimension n * m. eg n rows with each row having m number of columns\n",
    "\n",
    "\n",
    "2. You create a random weight matrix of any dimensions ,say (p, j) . Each cell in the weight matrix had m number of values. so essentially you create a random weight matrix of (p,j,m).\n",
    "\n",
    "\n",
    "3. normalize your data set within values of 0 - 1. In this example we will be using sklearn.preprocessing.MinMaxScaler\n",
    "\n",
    "\n",
    "4. At this time we will provide a couple of hyperparameters, as given in the below steps\n",
    "\n",
    "\n",
    "5. Provide a value for number of iterations eg 400/2000,etc\n",
    "\n",
    "\n",
    "6. Provide a learning rate. You may give a learning rate that remains constant through out the training phase, or you may chose to have a decaying learning rate( this is generally the case, and also what we will be using in our example below). IN case you provide a decaying learning rate, you will have to provide a range of learning rate. eg a starting learning rate and a lower learning range .\n",
    "\n",
    "\n",
    "7. If you are using decaying learning rate technique, we will also be giving a iteration count, till which we would be decaying our learning rate\n",
    "\n",
    "\n",
    "8. We will also be giving a starting radius, which will decide our sphere of influence. More on this later.\n",
    "\n",
    "\n",
    "9. Our last parameter would be tau parameter, which is a decaying degree. This will define how much our radius or learning rate will decay\n",
    "\n",
    "\n",
    "10. We will run the following number of iteration times:\n",
    "\n",
    "      -select a random x from our dataset\n",
    "      \n",
    "      -for each weight in the weight matrix, calculate the distance(could be euclidean,eg)\n",
    "      \n",
    "      -select the weight matrix with the least distance from x. Lets say this weight is W, known as BEST MATCHING UNIT\n",
    "      \n",
    "      -based on the radius parameter, calculate the neurons around W which falls within this circumference\n",
    "      \n",
    "      -based on the learning rate, recalculate the weights within the sphere of influence\n",
    "      \n",
    "      -recalculation will be : w = w + learning_rate*neighborhood_function(radius)*(x_random-w)\n",
    "      \n",
    "      -neighborhood function will be  exp(-distance/2 * squared(radius), where distance is the distance between W and this particular weight\n",
    "      \n",
    " \n",
    "11. At the end of predecided number of iterations, we will have a network of weights. The weights have been dragged, recalculated to resemble similarities between original dataset. leading to clustering of patterns together \n",
    "\n",
    "\n",
    "our learning rate decaying will work as:\n",
    "\n",
    "if iteration< itration_max\n",
    "\n",
    "    new_learning_rate = starting_learning_rate * exp(-iteration/tau)\n",
    "    \n",
    "else\n",
    "\n",
    "    new_learning_rate = end_learning_rate\n",
    "            \n",
    "            \n",
    "our radius will decay as given:\n",
    "\n",
    "\n",
    "radius = radius * exp(- iteration/tau )\n",
    "\n",
    "\n",
    "\n",
    "More details can be found in the following resources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Self-organizing_map\n",
    "\n",
    "https://medium.com/@navdeepsingh_2336/self-organizing-maps-for-machine-learning-algorithms-ad256a395fc5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets proceed to creating some of the utility function that will be used before we write the SOM algorithm\n",
    "\n",
    "#### significant amount of information has been provided in the doc strings. HOpe you find them useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a method to generate input\n",
    "def create_input_data(number_of_rows, number_of_columns = 3):\n",
    "    \"\"\"\n",
    "\n",
    "        Method returns a 3D data equal to number of rows.\n",
    "        in this method we wil return a range of colors\n",
    "        so each row will have random values for 0-255\n",
    "        for red green blue channel\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return np.random.randint(0,255, (number_of_rows, number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## our distance measure method. we will be using this to calculate distance between random point in dataset and the neuron.\n",
    "\n",
    "## We will also be using this to identify the weights around the Best Matching Unit, which will undergo reweighing\n",
    "\n",
    "def calculate_distance(a, b):\n",
    "    \"\"\"\n",
    "    this method would calculate the euclidean distance between\n",
    "    points a and b.\n",
    "\n",
    "    essentially this will serve as a distance measure between our\n",
    "    node and weight neuron.\n",
    "\n",
    "    \"\"\"\n",
    "    ## sqrt(np.sum(a-b))\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_input_data(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized input to the map to a range of 0-1.\n",
    "    SOMs work best under scaled inputs.\n",
    "    Using scikit learn MinMaXScaler. else we could have\n",
    "    done manually as well.\n",
    "\n",
    "    \"\"\"\n",
    "    return MinMaxScaler(feature_range = (0,1)).fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## this particular method will calculate distance between a randomly selected X value from data set\n",
    "## and the neurons in the weight matrix.This will identify the lowest weight neuron(BEST MATCHING UNIT) and return the index of the weight matrix\n",
    "\n",
    "## the intuition behind this is: lowest weight is the most resembling the dataset selected\n",
    "def compare_node_with_weight_matrix(x, w):\n",
    "\n",
    "    _units = {}\n",
    "    \n",
    "    for i, j in enumerate(w):\n",
    "        count = 0\n",
    "        \n",
    "        for neuron in j:\n",
    "\n",
    "            _units[(i,count)] = calculate_distance( x, neuron )\n",
    "            count += 1\n",
    "\n",
    "    ## sorting the dictionary according to value of each tuple\n",
    "    ## where each tuple represents the co ordinates\n",
    "    return sorted(_units, key = lambda x: _units[x])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is our neighborhood function. this will be used to remeasure our weights. the intuition behind this is to slowly reduce\n",
    "## the sphere of influence so that with more number of iteration, we remeasure more precise number of \n",
    "## neurons around the best matching unit\n",
    "\n",
    "def calculate_degree_of_influence(radius, distance):\n",
    "    \"\"\"\n",
    "\n",
    "    :param radius: the decayed value of radius\n",
    "    :param distance: the distance between current weight cell and the best matching unit\n",
    "\n",
    "    \"\"\"\n",
    "    denom = 2 * (radius)**2\n",
    "    if denom > np.finfo(np.float).eps: ## this prevents divide by zero issue. in case number is close to 0, we simply return 0\n",
    "        return np.exp(- distance/denom) \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_learning_rate(learning_rate, iteration_max, iteration, tau, final_learning_rate):\n",
    "    \"\"\"\n",
    "        learning rate will start decaying and reach a limit after iteration\n",
    "        reaches iteration_max value\n",
    "\n",
    "        if learning rate needs to be decayed, we need to pass not only the\n",
    "        iteration max as a non zero value, we would also need to pass\n",
    "        the range of learning rate\n",
    "        eg: learning_rate = (0.1 , 0.02) \n",
    "\n",
    "        the way it will work is\n",
    "\n",
    "        if iteration< itration_max\n",
    "            new_learning_rate = learning_rate[0] * exp(-iteration/tau)\n",
    "        else\n",
    "            new_learning_rate = learning_rate[1]\n",
    "\n",
    "    \"\"\"       \n",
    "    \n",
    "    if iteration_max > 0 and final_learning_rate:\n",
    "        \n",
    "        if iteration < iteration_max:\n",
    "            return learning_rate * np.exp(-iteration/tau)\n",
    "    \n",
    "        else:\n",
    "\n",
    "            return final_learning_rate\n",
    "\n",
    "    else:\n",
    "        return learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_radius(radius, time_decay, iteration):\n",
    "    \"\"\"\n",
    "        time decay operation on the initial radius value given\n",
    "\n",
    "    \"\"\"\n",
    "    return radius * np.exp(- iteration/time_decay )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## we will use this method to visualize the final color grid\n",
    "\n",
    "def plot_in_self_organized_map(net):\n",
    "    fig = plot.figure()\n",
    "    # setup axes\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.set_xlim((0, net.shape[0]+1))\n",
    "    ax.set_ylim((0, net.shape[1]+1))\n",
    "    ax.set_title('Self-Organising Map after iterations')\n",
    "\n",
    "    # plot the rectangles\n",
    "    for x in range(1, net.shape[0] + 1):\n",
    "        for y in range(1, net.shape[1] + 1):\n",
    "            ax.add_patch(patches.Rectangle((x-0.5, y-0.5), 1, 1,\n",
    "                        facecolor=net[x-1,y-1,:],\n",
    "                        edgecolor='none'))\n",
    "    plot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(x, _best_matching_index, weight_matrix, \\\n",
    "                    learning_rate, tau, sigma, iteration, \\\n",
    "                        iteration_max, final_learning_rate):\n",
    "    \"\"\"\n",
    "\n",
    "        how are the weights updated.\n",
    "        based on sigma parameter, we select the sphere of influence\n",
    "        w = w + learning_rate*neighborhood_function(sigma)*(x_random-w)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _best_matching_unit = weight_matrix[_best_matching_index[0]][_best_matching_index[1]]\n",
    "    radius = decay_radius(sigma, tau, iteration)\n",
    "\n",
    "    for i, j in enumerate(weight_matrix):\n",
    "        count = 0\n",
    "        for neuron in j:\n",
    "\n",
    "            distance = calculate_distance( np.reshape(_best_matching_index,2), np.reshape((i,count),2) )\n",
    "\n",
    "            if distance <= radius**2: ## within sphere of influence\n",
    "                \n",
    "                degree_of_influence = calculate_degree_of_influence(radius, distance)\n",
    "                \n",
    "                learning_rate = calculate_learning_rate(learning_rate, iteration_max, \\\n",
    "                                    iteration, tau, final_learning_rate)\n",
    "                \n",
    "                new_weight = neuron + (learning_rate * degree_of_influence * (x - neuron))\n",
    "\n",
    "                # neuron = new_weight\n",
    "\n",
    "                weight_matrix[i][count] = new_weight\n",
    "            \n",
    "            count+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## our goal will be to visualize the above 3channel 3d color as a \n",
    "## 2D color map, with similar colors grouping together closely.\n",
    "## so reds closes to reds, greens closer to green\n",
    "## we will use a self organizing map to act as a dimensionality reduction\n",
    "## technique. once reduced the network would be plotted on matplotlib\n",
    "## to check if we have actually reduced the dimensions\n",
    "\n",
    "def self_organize_map(x, size_of_grid, epoch, learning_rate, tau, \\\n",
    "                        sigma, final_learning_rate = 0.02, iteration_max = 0):\n",
    "    \"\"\"\n",
    "        runs a SOM on the given input data.\n",
    "\n",
    "        :param x: the scaled input to the network\n",
    "\n",
    "        :param size_of_grid: the size of the weight matrix. expecting a tuple (k,p)\n",
    "                             remember the weight matrix is k*p*m dimension\n",
    "                             where each k*p cell is associated with a vector\n",
    "                             of length m\n",
    "\n",
    "        :param epoch: int value depicting total number of iterations to be executed\n",
    "\n",
    "        :param learning_rate: learning rate. for now we will use the same learning rate\n",
    "                              across all iterations. however generally a decaying learning \n",
    "                              rate performs better\n",
    "\n",
    "        :param tau: tau works as a decaying parameter. we will use the same tau parameter to\n",
    "                    decay the neighbor hood radius as well as learning rate if applicable\n",
    "\n",
    "        :param sigma: the sigma value will be used to depict the starting radius\n",
    "                      after every iteration we will decay the radius using the tau parameter\n",
    "\n",
    "        :param final_learning_rate: if learning rate needs to be decayed this is the value we will\n",
    "                                take after iteration count exceeds the iteration_max\n",
    "\n",
    "        :param iteration_max: this parameter will be used to dictate from which particular \n",
    "                            iteration the learning rate will stop decaying\n",
    "    \"\"\"\n",
    "    \n",
    "    dimensionality_of_input = x.shape[1]\n",
    "    \n",
    "    ## random weight matrix initialized.\n",
    "    weight_matrix = np.random.rand(size_of_grid[0],size_of_grid[1],dimensionality_of_input)\n",
    "\n",
    "    # sigma = max(weight_matrix[0], weight_matrix[1]) / 2\n",
    "    for _iteration in range(epoch):\n",
    "\n",
    "        ## select a random x input\n",
    "        x_random = x[random.randint(0, x.shape[0]-1)]\n",
    "\n",
    "        ## compare values of weight matrix with the above selected x_random\n",
    "        ## we will select the closest neuron - best matching unit BMU\n",
    "        _best_matching_index = compare_node_with_weight_matrix(x_random,weight_matrix)\n",
    "\n",
    "        ## once our BMU is selected. we will update the weights around it\n",
    "        ## based on the neighborhood function as sigma parameter(As radius)\n",
    "        ## and tau parameter( as decaying option)\n",
    "\n",
    "        update_weights(x_random,_best_matching_index, \\\n",
    "                            weight_matrix,learning_rate, \\\n",
    "                                tau, sigma, _iteration, iteration_max, final_learning_rate)\n",
    "\n",
    "    ## our weight_matrix is the network which has learnt the behavior and\n",
    "    ## correlations between the values. we will plot this grid on a matplot\n",
    "    ## for visualization aid\n",
    "\n",
    "    return weight_matrix    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute():\n",
    "\n",
    "    x = scale_input_data(create_input_data(100)) ## we create a 100 row, 3d color map and scale it\n",
    "\n",
    "    epoch = 2000\n",
    "    weight_shape = (10,10)\n",
    "    learning_rate = 0.5\n",
    "    tau = 1000\n",
    "    sigma = 5 ## initial radius will be half way of the weight matrix dimension (5x,5y) /2 = 2.5\n",
    "\n",
    "    weight_matrix = self_organize_map(x, weight_shape,epoch,learning_rate,tau,sigma, iteration_max= 200)\n",
    "\n",
    "    plot_in_self_organized_map(weight_matrix)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somak\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot as plot \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see above, we had an input of 100 rows of RGB values. SOM reduced the dimensions to 2D grid, and at the same time , colors of similar nature have clustered together to form a pattern "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
